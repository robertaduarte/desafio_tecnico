# Desafio T√©cnico

Nesse reposit√≥rio foi implementado o ESRGAN (_Enhanced Super-Resolution Generative Adversarial Network_), um modelo baseado em redes adversariais generativas que √© eficaz e tem uma literatura robusta em tarefas de refinamento e super-resolu√ß√£o de imagens.

### O que √© ESRGAN?
ESRGAN √© projetada para produzir imagens de super-resolu√ß√£o. Diferente da SRGAN, a ESRGAN introduz blocos residuais que aumentam a estabilidade e profundidade, o uso de uma _loss function_ perceptual que usa _features_ extra√≠das de redes pr√©-treinadas (VGG, nesse caso) e um discriminador mais robusto. A ESRGAN √© baseada no artigo **[ESRGAN: Enhanced Super-Resolution Generative Adversarial Networks](https://arxiv.org/abs/1809.00219)** do Wang et al. 2018.


A escolha de uso da ESRGAN nesse desafio √© devido aos casos bem sucedidos de aplica√ß√£o para super resolu√ß√£o em 2x, 4x e 8x. Al√©m disso, o discriminador retorna um valor probabil√≠stico de semelhan√ßa entre target e imagem gerada pela geradora. A facilidade de aumentar a ESRGAN para diferentes tarefas como, por exemplo, a geradora retornar _N_ imagens cada qual com diferentes camadas de sa√≠da, tamb√©m √© um motivo para utilizar a ESRGAN com esse intuito.

<p align="center">
  <img src="https://github.com/user-attachments/assets/129d6e2a-fa75-4935-a999-93459bc5f4fe" alt="Architecture-of-ESRGAN" width="700"/>
</p>

### Arquitetura

A estrutura principal da ESRGAN, como vista na imagem acima, √© composta por tr√™s componentes: _generator_, _discriminator_ e uma _perceptual loss_. Abaixo est√° a descri√ß√£o detalhada da arquitetura tradicional da ESRGAN:

‚û°Ô∏è ***Generator***

O _generator_ √© projetado para aumentar a resolu√ß√£o de imagens de forma realista. Ele √© baseado em tr√™s partes principais:

1. _First Convolutional Layer_: uma camada convolucional com kernel 3x3 que projeta a imagem de entrada para o _feature space_.

2. _Residual-in-Residual Dense Block (RRDB)_: cada RRDB cont√©m tr√™s _Dense Blocks_ conectados com conex√µes residuais internas e externas: camadas _DenseNet_ que recebem como entrada as camadas anteriores, sem uso de _batch normalization_, fun√ß√£o de ativa√ß√£o _LeakyReLU_ e _residual scaling_ com fator de 0.2 para evitar instabilidades.

3. _Upsampling Layers_: uma camada de _PixelShuffle_ que dobram a resolu√ß√£o a cada passo seguida por uma ativa√ß√£o _LeakyReLU_.

4. _Output Convolution_: Uma √∫ltima convolu√ß√£o 3x3 que projeta o tensor final de volta para o n√∫mero de canais da imagem RGB (3).

‚û°Ô∏è ***Discriminator*** 

O _discriminator_ da ESRGAN √© baseado na _PatchGAN_ que avalia se _patches_ da imagem s√£o reais ou falsos.

1. Conjunto de camadas convolucionais com stride 1 e 2 alternados, seguidos de LeakyReLU.

2. Finaliza com fully connected layers ou uma Adaptive Pooling + 1√ó1 conv para sa√≠da escalar.

‚û°Ô∏è ***Perceptual Loss*** 

A _loss function_ do _generator_ combina: _pixel-wise loss_, _perceptual loss_ (VGG) e _adversarial loss_.

# Meu Modelo

O meu modelo segue o formato de uma ESRGAN tradicional com algumas altera√ß√µes espec√≠ficas para o problema. Abaixo, descreverei as mudan√ßas que foram feitas em cada arquivo e os problemas que resultaram nessas mudan√ßas:

`architecture.py`: a camada de arquitetura foi implementada seguindo os passos do artigo  **[Wang et al. 2018](https://arxiv.org/abs/1809.00219)**. 

1. A primeira mudan√ßa est√° nas √∫ltimas camadas da geradora que foram alteradas para retornar 3 imagens em vez de 1 s√≥. Cada imagem √© resultado de uma camada de sa√≠da diferente, mais especificamente:
    - Imagem 1: a camada final √© um bloco onde tem duas vezes uma convolu√ß√£o 9x9 seguida de uma _LeakyReLU_ e uma convolu√ß√£o final 9x9.
    - Imagem 2: a camada final √© uma convolu√ß√£o 9x9 seguida de uma _LeakyReLU_ e uma convolu√ß√£o final 9x9.
    - Imagem 3: a camada final √© uma convolu√ß√£o 9x9.
      
2. A segunda mudan√ßa √© no adicional de um condicional de escalas porque uma escala de 2 precisa de apenas 1 _Upsampling Layer_ enquanto uma escala de 8 precisa de 3 _Upsampling Layers_. Cada _Upsampling Layer_ aumenta a imagem em 2x. Ent√£o dependendo da condicional que √© recebida na classe da _generator_, o modelo se organiza para ser treinado com a escala escolhida.

3. Adicionei uma convolu√ß√£o 3x3 antes da camada _PixelShuffle_ para refinar o treinamento.

üö® ***Importante***: Apenas a arquitetura para escala `8x` √© ligeiramente diferente por quest√µes de custo computacional. A arquitetura pode ser encontrada na pasta `modelo_RAM`.

`perceptual_loss.py`: implementa a _perceptual loss_ baseada no modelo pr√©-treinado VGG para comparar as features.

`data.py`: o arquivo possui fun√ß√µes que tratam os dados e preparam batches, um de cada vez. Apesar da prepara√ß√£o em loop, esse formato √© eficiente para n√£o gastar a RAM quando todos dados s√£o guarados de uma vez em um _dataloader_ (na pasta `modelo_RAM` tem um `data.py` que cria um _dataloader_ que pode ser eficiente para dados menores).

`train.py`: o arquivo possui o loop de treinamento que considera batches separados. Al√©m disso, primeiro a discriminadora √© treinada e depois a geradora √© treinada onde ela devolve 3 imagens que s√£o comparadas com o target e a _loss function_ √© somada. Ao final, o modelo chama o arquivo `evaluation.py` que plota os resultados ao final de cada √©poca assim como as m√©tricas de valida√ß√£o.

`evaluation.py`: arquivo realiza a avalia√ß√£o em cima das imagens de valida√ß√£o e retorna a imagem gerada no diret√≥rio, a imagem possui as 3 imagens geradas pela _generator_ assim como as m√©tricas MSE, SSIM e PNSR para cada uma em compara√£o ao target.

`score.py`: retorna as m√©tricas e rankeia da melhor para pior imagem al√©m de dar qual √© a mais prov√°vel de ser confundida com target

`requirements.txt`: todos requisitos para rodar o c√≥digo

### Hiperpar√¢metros

Nesse trabalho, os hiperpar√¢metros foram:

1. N√∫mero de camadas de convolu√ß√£o na sa√≠da da geradora para cada uma das 3 imagens
2. N√∫mero de filtros
3. _Batch size_

Como a ESRGAN tem uma literatura robusta e bem estabelecida, diversos hiperpar√¢metros tradicionais (como _learning rate_, pesos das loss, etc) j√° eram estabelecidos como _default_ para esse caso.

### M√©tricas

As m√©tricas utilizadas para avaliar o resultado na parte de valida√ß√£o s√£o m√©tricas usadas tradicionalmente em an√°lises de problemas de vis√£o computacional:

1.  MSE (Mean Squared Error): avalia, em valor absoluto, a diferen√ßa entre a imagem gerada e o _target_ como a m√©dia dos _pixels_.
2.  PSNR (Peak Signal-to-Noise Ratio): representa a qualidade da imagem gerada em rela√ß√£o ao _target_.
3.  SSIM (Structural Similarity Index Measure): avalia se as imagens t√™m a mesma estrutura e apar√™ncia visual.

### Overfitting

Para evitar o overfitting, a _perceptual loss_ ajuda for√ßando o modelo a aprender representa√ß√µes visuais que j√° foram pr√©-treinadas em outra base de dados e n√£o a memorizar pixel a pixel. Outro ponto √© que as GANs, em geral, evitam o overfitting gra√ßas a _discriminator_, isso porque conforme o _discriminator_ tamb√©m √© aprendido durante o processo e √© necess√°rio que imagens mais realistas sejam criadas para diferentes cen√°rios. A presen√ßa do bloco RRDB tamb√©m contribui para evitar o _overfitting_ porque a rede fica mais est√°vel e generaliz√°vel com as conex√µes no RRDB. 

# Resultados

Os resultados podem ser encontrados na pasta `imagens` onde dentro possui 3 pastas com nomes `2x`, `4x` e `8x` correspondendo ao _upscale_ associado assim como o arquivo `inference.py` que originou cada imagem. Cada imagem possui as m√©tricas MSE, PSNR, SSIM em cima e a √∫ltima columa representa o target.

As escalas usadas foram (veja a se√ß√£o Limita√ß√µes):

-  2x: 256 x 128 para 512 x 256
-  4x: 128 x 64 para 512 x 256
-  8x: 128 x 64 para 1024 x 512

Alguns pontos a serem discutidos:

1. Todos os modelos conseguiram recuperar bem as estruturas e cores da imagem conforme o modelo foi sendo treinado. √â poss√≠vel observar isso nas imagens que v√°rias n√£o possuem diferen√ßas a olho nu e as diferen√ßas s√≥ s√£o capturados pelas m√©tricas correspondendo.
2. Como esperado, o _upscale_ de `8x` foi o mais desafiador e com os resultados inferiores aos de `2x` e `4x`. Isso se deve a necessidade de uma arquitetura maior que seja capaz de capturar os detalhes mais finos das imagens. Apesar disso, algumas das imagens geradas pelo modelo treinado `8x` alcan√ßam SSIM de 0.8 indicando uma boa semelhan√ßa com o _target_.
3. `2x` e `4x` possuem resultados semelhantes e tamb√©m variam o valor do SSIM que ficam por volta de 0.7 at√© 0.8. Isso reflete tamb√©m a resolu√ß√£o da imagem de _output_ que n√£o possui uma resolu√ß√£o t√£o alta (512 x 256) fazendo com que o modelo n√£o aprenda bem detalhes mais finos.

Os pesos para cada modelo est√£o na pasta `pesos` onde os arquivos  `scale_2x.pth`, `scale_4x.pth` e `scale_8x.pth` s√£o para `2x`, `4x` e `8x`, respectivamente.

# Limita√ß√µes

1. **Custo computacional elevado**:
  -  Impediu o uso de imagens em resolu√ß√£o 2K, limitando o treinamento a resolu√ß√µes mais baixas.
  -  Isso comprometeu o aprendizado de detalhes finos, importantes para a reconstru√ß√£o de imagens mais realistas.

2. **Arquitetura simplificada**:

  - Devido √†s restri√ß√µes de hardware, n√£o foi poss√≠vel testar vers√µes mais profundas da rede, com mais camadas ou blocos residuais, o que pode ter limitado a capacidade do modelo aprender alguns detalhes, principalmente no caso de `8x`.

3. **Tempo de processamento por √©poca**:

  - O treinamento lento dificultou o uso de um n√∫mero maior de √©pocas, isso dificultou uma converg√™ncia melhor.

# Aprimoramento

Uma lista de poss√≠veis aprimoramentos s√£o:

1. Adicionar _data augmentation_.
2. Rodar por mais √©pocas para uma converg√™ncia melhor (os modelos n√£o treinaram por muito tempo, apenas cerca de 50 a 80 √©pocas por quest√µes computacionais)
3. Treinar com _inputs_ que recebem ru√≠do na entrada.
4. Testar 3 _generators_ no mesmo modelo em vez de mudar apenas a sa√≠da de 1 _generator_.
5. Adicionar uma _ranked loss_ que penaliza a pior imagem gerada e refor√ßa a melhor.
6. Como parte da arquitetura √© condicional, treinar com diferentes escalas ao mesmo tempo.
7. Adicionar camadas de aten√ß√£o na _generator_.
8. Fazer treinamento em paralelo no espa√ßo de Fourier para pegar diferentes escalas.

## Sandbox

Na pasta,  `data_RAM` h√° um file `data.py` que exemplifica o uso de um _dataloader_ nesse caso. _dataloader_ √© interessante para caso de paraleliza√ß√£o dos dados. 
